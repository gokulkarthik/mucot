/home/gokul.kumar/.conda/envs/mlqa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 29821
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 8
  Total optimization steps = 2796
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"




















































































































































































































 18%|████████████████▌                                                                            | 499/2796 [07:04<32:44,  1.17it/s]
























































































































































































 33%|███████████████████████████████                                                              | 932/2796 [13:16<26:11,  1.19it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







 96%|██████████████████████████████████████████████████████████████████████████████████████████▍   | 408/424 [00:13<00:00, 29.50it/s]

Configuration saved in ckpts/checkpoint-932/config.json
Model weights saved in ckpts/checkpoint-932/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-932/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-932/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-1864] due to args.save_total_limit




























 36%|████████████████████████████████▉                                                           | 1000/2796 [14:34<26:35,  1.13it/s]






















































































































































































































 54%|█████████████████████████████████████████████████▎                                          | 1500/2796 [21:42<19:15,  1.12it/s]



























































































































































 67%|█████████████████████████████████████████████████████████████▎                              | 1864/2796 [26:54<13:00,  1.19it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







Configuration saved in ckpts/checkpoint-1864/config.json
{'eval_loss': 0.33364614844322205, 'eval_runtime': 14.3537, 'eval_samples_per_second': 117.949, 'eval_steps_per_second': 29.539, 'epoch': 2.0}
Model weights saved in ckpts/checkpoint-1864/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-1864/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-1864/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-2796] due to args.save_total_limit


























































 72%|█████████████████████████████████████████████████████████████████▊                          | 2000/2796 [29:11<11:53,  1.12it/s]





















































































































































































































 89%|██████████████████████████████████████████████████████████████████████████████████▏         | 2498/2796 [36:18<04:15,  1.17it/s]






























































































































100%|████████████████████████████████████████████████████████████████████████████████████████████| 2796/2796 [40:33<00:00,  1.19it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







 96%|██████████████████████████████████████████████████████████████████████████████████████████    | 406/424 [00:13<00:00, 29.47it/s]

Configuration saved in ckpts/checkpoint-2796/config.json
Model weights saved in ckpts/checkpoint-2796/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-2796/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-2796/special_tokens_map.json
{'train_runtime': 2454.5278, 'train_samples_per_second': 36.448, 'train_steps_per_second': 1.139, 'train_loss': 0.6054777382781066, 'epoch': 3.0}
Deleting older checkpoint [ckpts/checkpoint-932] due to args.save_total_limit
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from ckpts/checkpoint-2796 (score: 0.32688671350479126).
100%|████████████████████████████████████████████████████████████████████████████████████████████| 2796/2796 [40:54<00:00,  1.14it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:36<00:00, 48.25s/ba]
The following columns in the test set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.
***** Running Prediction *****
  Num examples = 29821
  Batch size = 4
  1%|▌                                                                                             | 42/7456 [00:01<04:16, 28.93it/s]







































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████| 7456/7456 [04:38<00:00, 27.86it/s]
Post-processing 1645 example predictions split into 29821 features.













Traceback (most recent call last):███████████████████████████▏                                    | 994/1645 [00:28<00:16, 39.77it/s]
  File "/home/gokul.kumar/Desktop/multilingual-qa/main.py", line 154, in <module>
    main(args)
  File "/home/gokul.kumar/Desktop/multilingual-qa/main.py", line 106, in main
    train_predictions = postprocess_qa_predictions(dataset_train, dataset_train_tokenized,
  File "/home/gokul.kumar/Desktop/multilingual-qa/datasets_local.py", line 226, in postprocess_qa_predictions
    end_char = offset_mapping[end_index][1]
IndexError: list index out of range
0
384
0
[[], [], [], [], [], [], [], [], [], []]