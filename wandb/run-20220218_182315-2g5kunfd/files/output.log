/home/gokul.kumar/.conda/envs/mlqa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 29821
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 8
  Total optimization steps = 2796
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"





















































































































































































































 18%|████████████████▌                                                                            | 499/2796 [07:06<32:45,  1.17it/s]

























































































































































































 33%|███████████████████████████████                                                              | 932/2796 [13:18<26:08,  1.19it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4






 92%|██████████████████████████████████████████████████████████████████████████████████████▋       | 391/424 [00:13<00:01, 29.51it/s]

Configuration saved in ckpts/checkpoint-932/config.json
Model weights saved in ckpts/checkpoint-932/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-932/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-932/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-2284] due to args.save_total_limit




























 36%|█████████████████████████████████▏                                                           | 998/2796 [14:35<25:36,  1.17it/s]























































































































































































































 54%|█████████████████████████████████████████████████▎                                          | 1500/2796 [21:46<19:24,  1.11it/s]



























































































































































 67%|█████████████████████████████████████████████████████████████▎                              | 1864/2796 [26:58<13:02,  1.19it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







Configuration saved in ckpts/checkpoint-1864/config.json
{'eval_loss': 0.3409470021724701, 'eval_runtime': 14.3597, 'eval_samples_per_second': 117.899, 'eval_steps_per_second': 29.527, 'epoch': 2.0}
Model weights saved in ckpts/checkpoint-1864/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-1864/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-1864/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-3426] due to args.save_total_limit


























































 72%|█████████████████████████████████████████████████████████████████▊                          | 2000/2796 [29:15<11:50,  1.12it/s]























































































































































































































 89%|██████████████████████████████████████████████████████████████████████████████████▎         | 2502/2796 [36:26<04:18,  1.14it/s]





























































































































100%|████████████████████████████████████████████████████████████████████████████████████████████| 2796/2796 [40:39<00:00,  1.19it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







 99%|████████████████████████████████████████████████████████████████████████████████████████████▋ | 418/424 [00:14<00:00, 29.52it/s]
Configuration saved in ckpts/checkpoint-2796/config.json
Model weights saved in ckpts/checkpoint-2796/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-2796/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-2796/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-932] due to args.save_total_limit
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from ckpts/checkpoint-2796 (score: 0.3344261646270752).
{'train_runtime': 2460.0521, 'train_samples_per_second': 36.366, 'train_steps_per_second': 1.137, 'train_loss': 0.5998873812957894, 'epoch': 3.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████| 2796/2796 [41:00<00:00,  1.14it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:39<00:00, 49.79s/ba]
The following columns in the test set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.
***** Running Prediction *****
  Num examples = 29821
  Batch size = 4
  0%|▎                                                                                             | 28/7456 [00:00<04:15, 29.05it/s]







































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████| 7456/7456 [04:47<00:00, 27.69it/s]
Post-processing 1645 example predictions split into 29821 features.













Traceback (most recent call last):███████████████████████████▏                                    | 994/1645 [00:29<00:16, 39.21it/s]
  File "/home/gokul.kumar/Desktop/multilingual-qa/main.py", line 154, in <module>
    main(args)
  File "/home/gokul.kumar/Desktop/multilingual-qa/main.py", line 106, in main
    train_predictions = postprocess_qa_predictions(dataset_train, dataset_train_tokenized,
  File "/home/gokul.kumar/Desktop/multilingual-qa/datasets_local.py", line 219, in postprocess_qa_predictions
    start_char = offset_mapping[start_index][0]
IndexError: list index out of range