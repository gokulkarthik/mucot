/home/gokul.kumar/.conda/envs/mlqa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 18290
  Num Epochs = 6
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 8
  Total optimization steps = 3426
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"




















































































































































































































 15%|█████████████▌                                                                               | 498/3426 [07:04<41:49,  1.17it/s]































 17%|███████████████▌                                                                             | 571/3426 [08:07<40:51,  1.16it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4






 94%|████████████████████████████████████████████████████████████████████████████████████████▍     | 399/424 [00:13<00:00, 29.47it/s]

Configuration saved in ckpts/checkpoint-571/config.json
Model weights saved in ckpts/checkpoint-571/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-571/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-571/special_tokens_map.json























































































































































































 29%|██████████████████████████▊                                                                 | 1000/3426 [14:36<36:05,  1.12it/s]




























































 33%|██████████████████████████████▋                                                             | 1142/3426 [16:38<32:35,  1.17it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







 99%|████████████████████████████████████████████████████████████████████████████████████████████▋ | 418/424 [00:14<00:00, 29.48it/s]

Configuration saved in ckpts/checkpoint-1142/config.json
Model weights saved in ckpts/checkpoint-1142/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-1142/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-1142/special_tokens_map.json

























































































































































 44%|████████████████████████████████████████▎                                                   | 1501/3426 [22:06<28:21,  1.13it/s]


























































































 50%|██████████████████████████████████████████████                                              | 1713/3426 [25:08<24:30,  1.16it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







100%|█████████████████████████████████████████████████████████████████████████████████████████████▌| 422/424 [00:14<00:00, 29.32it/s]
Configuration saved in ckpts/checkpoint-1713/config.json
Model weights saved in ckpts/checkpoint-1713/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-1713/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-1713/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-571] due to args.save_total_limit


























































































































 58%|█████████████████████████████████████████████████████▋                                      | 2000/3426 [29:36<20:59,  1.13it/s]

























































































































 67%|█████████████████████████████████████████████████████████████▎                              | 2284/3426 [33:40<16:21,  1.16it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







Configuration saved in ckpts/checkpoint-2284/config.json
Model weights saved in ckpts/checkpoint-2284/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-2284/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-2284/special_tokens_map.json
{'eval_loss': 0.3225606381893158, 'eval_runtime': 14.3738, 'eval_samples_per_second': 117.784, 'eval_steps_per_second': 29.498, 'epoch': 4.0}
Deleting older checkpoint [ckpts/checkpoint-1142] due to args.save_total_limit




























































































 73%|███████████████████████████████████████████████████████████████████▏                        | 2501/3426 [37:07<13:33,  1.14it/s]























































































































































 83%|████████████████████████████████████████████████████████████████████████████▋               | 2855/3426 [42:11<08:09,  1.17it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







 99%|████████████████████████████████████████████████████████████████████████████████████████████▋ | 418/424 [00:14<00:00, 29.44it/s]

Configuration saved in ckpts/checkpoint-2855/config.json
Model weights saved in ckpts/checkpoint-2855/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-2855/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-2855/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-1713] due to args.save_total_limit





























































 88%|████████████████████████████████████████████████████████████████████████████████▌           | 2999/3426 [44:35<06:13,  1.14it/s]






















































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████| 3426/3426 [50:42<00:00,  1.17it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







Configuration saved in ckpts/checkpoint-3426/config.json
{'eval_loss': 0.3258655071258545, 'eval_runtime': 14.3888, 'eval_samples_per_second': 117.661, 'eval_steps_per_second': 29.467, 'epoch': 6.0}
Model weights saved in ckpts/checkpoint-3426/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-3426/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-3426/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-2855] due to args.save_total_limit
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from ckpts/checkpoint-2284 (score: 0.3225606381893158).
100%|████████████████████████████████████████████████████████████████████████████████████████████| 3426/3426 [51:03<00:00,  1.12it/s]
{'train_runtime': 3063.4866, 'train_samples_per_second': 35.822, 'train_steps_per_second': 1.118, 'train_loss': 0.5309294963954696, 'epoch': 6.0}

 50%|█████████████████████████████████████████████████                                                 | 1/2 [00:07<00:07,  7.16s/ba]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.70s/ba]
***** Running Prediction *****
  Num examples = 18290
  Batch size = 4

















































































100%|███████████████████████████████████████████████████████████████████████████████████████████▉| 4572/4573 [02:45<00:00, 27.71it/s]Traceback (most recent call last):
  File "/home/gokul.kumar/Desktop/multilingual-qa/main.py", line 154, in <module>
    main(args)
  File "/home/gokul.kumar/Desktop/multilingual-qa/main.py", line 106, in main
    train_predictions = postprocess_qa_predictions(dataset_train, dataset_train_tokenized,
  File "/home/gokul.kumar/Desktop/multilingual-qa/datasets_local.py", line 169, in postprocess_qa_predictions
    features_per_example[example_id_to_index[feature["example_id"]]].append(i)
KeyError: 'example_id'