/home/gokul.kumar/.conda/envs/mlqa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 29821
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 8
  Total optimization steps = 2796
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"









































  4%|███▎                                                                                         | 100/2796 [01:23<38:02,  1.18it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4






 91%|█████████████████████████████████████████████████████████████████████████████████████▏        | 384/424 [00:12<00:01, 29.66it/s]











































  7%|██████▋                                                                                      | 200/2796 [03:03<36:59,  1.17it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4






 94%|████████████████████████████████████████████████████████████████████████████████████████▋     | 400/424 [00:13<00:00, 29.62it/s]











































 11%|█████████▉                                                                                   | 300/2796 [04:43<35:38,  1.17it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4






 95%|█████████████████████████████████████████████████████████████████████████████████████████▌    | 404/424 [00:13<00:00, 29.50it/s]











































 14%|█████████████▎                                                                               | 400/2796 [06:23<34:13,  1.17it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4






 92%|██████████████████████████████████████████████████████████████████████████████████████▏       | 389/424 [00:13<00:01, 29.40it/s]











































 18%|████████████████▌                                                                            | 499/2796 [08:03<32:46,  1.17it/s]
 18%|████████████████▋                                                                            | 500/2796 [08:04<34:08,  1.12it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4






 92%|██████████████████████████████████████████████████████████████████████████████████████▋       | 391/424 [00:13<00:01, 29.51it/s]

Configuration saved in ckpts/checkpoint-500/config.json
Model weights saved in ckpts/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-500/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-500/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-1864] due to args.save_total_limit









































 21%|███████████████████▉                                                                         | 600/2796 [09:50<31:26,  1.16it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4






 92%|██████████████████████████████████████████████████████████████████████████████████████▋       | 391/424 [00:13<00:01, 29.42it/s]











































 25%|███████████████████████▎                                                                     | 700/2796 [11:31<29:56,  1.17it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







 92%|██████████████████████████████████████████████████████████████████████████████████████▉       | 392/424 [00:13<00:01, 29.50it/s]










































 29%|██████████████████████████▌                                                                  | 800/2796 [13:11<28:30,  1.17it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







 89%|███████████████████████████████████████████████████████████████████████████████████▊          | 378/424 [00:12<00:01, 28.57it/s]










































 32%|█████████████████████████████▉                                                               | 900/2796 [14:51<27:09,  1.16it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







 90%|████████████████████████████████████████████████████████████████████████████████████▉         | 383/424 [00:12<00:01, 29.59it/s]










































 36%|████████████████████████████████▉                                                           | 1000/2796 [16:32<26:45,  1.12it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4
  7%|██████▍                                                                                        | 29/424 [00:00<00:13, 29.69it/s]







Configuration saved in ckpts/checkpoint-1000/config.json
{'eval_loss': 0.35320913791656494, 'eval_runtime': 14.3742, 'eval_samples_per_second': 117.78, 'eval_steps_per_second': 29.497, 'epoch': 1.07}
Model weights saved in ckpts/checkpoint-1000/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-1000/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-1000/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-2796] due to args.save_total_limit












