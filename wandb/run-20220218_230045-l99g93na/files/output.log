/home/gokul.kumar/.conda/envs/mlqa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 29821
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 8
  Total optimization steps = 2796
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"




















































































































































































































 18%|████████████████▌                                                                            | 499/2796 [07:04<32:43,  1.17it/s]
























































































































































































 33%|███████████████████████████████                                                              | 932/2796 [13:15<25:58,  1.20it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







Configuration saved in ckpts/checkpoint-932/config.json
{'eval_loss': 0.36478903889656067, 'eval_runtime': 14.3638, 'eval_samples_per_second': 117.866, 'eval_steps_per_second': 29.519, 'epoch': 1.0}
Model weights saved in ckpts/checkpoint-932/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-932/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-932/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-1864] due to args.save_total_limit




























 36%|████████████████████████████████▉                                                           | 1000/2796 [14:33<26:43,  1.12it/s]





















































































































































































































 54%|█████████████████████████████████████████████████▎                                          | 1499/2796 [21:40<18:28,  1.17it/s]



























































































































































 67%|█████████████████████████████████████████████████████████████▎                              | 1864/2796 [26:53<12:56,  1.20it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4







Configuration saved in ckpts/checkpoint-1864/config.json
{'eval_loss': 0.33165839314460754, 'eval_runtime': 14.3671, 'eval_samples_per_second': 117.839, 'eval_steps_per_second': 29.512, 'epoch': 2.0}
Model weights saved in ckpts/checkpoint-1864/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-1864/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-1864/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-2796] due to args.save_total_limit

























































 72%|█████████████████████████████████████████████████████████████████▊                          | 2000/2796 [29:09<11:48,  1.12it/s]





















































































































































































































 89%|██████████████████████████████████████████████████████████████████████████████████▏         | 2499/2796 [36:16<04:13,  1.17it/s]






























































































































100%|████████████████████████████████████████████████████████████████████████████████████████████| 2796/2796 [40:31<00:00,  1.20it/s]***** Running Evaluation *****
  Num examples = 1693
  Batch size = 4






 88%|██████████████████████████████████████████████████████████████████████████████████▍           | 372/424 [00:12<00:01, 29.43it/s]

Configuration saved in ckpts/checkpoint-2796/config.json
Model weights saved in ckpts/checkpoint-2796/pytorch_model.bin
tokenizer config file saved in ckpts/checkpoint-2796/tokenizer_config.json
Special tokens file saved in ckpts/checkpoint-2796/special_tokens_map.json
Deleting older checkpoint [ckpts/checkpoint-932] due to args.save_total_limit
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from ckpts/checkpoint-2796 (score: 0.3184095323085785).
{'train_runtime': 2451.8973, 'train_samples_per_second': 36.487, 'train_steps_per_second': 1.14, 'train_loss': 0.5650368064258232, 'epoch': 3.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████| 2796/2796 [40:51<00:00,  1.14it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:02<00:00, 31.43s/ba]
The following columns in the test set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.
***** Running Prediction *****
  Num examples = 18290
  Batch size = 4
  0%|▍                                                                                             | 19/4573 [00:00<02:34, 29.57it/s]


















































































100%|████████████████████████████████████████████████████████████████████████████████████████████| 4573/4573 [02:54<00:00, 27.98it/s]
Post-processing 1005 example predictions split into 18290 features.





















Traceback (most recent call last):██████████████████████████████████████████████████████████████▌| 1000/1005 [00:44<00:00, 24.10it/s]
  File "/home/gokul.kumar/Desktop/multilingual-qa/main.py", line 154, in <module>
    main(args)
  File "/home/gokul.kumar/Desktop/multilingual-qa/main.py", line 106, in main
    train_predictions = postprocess_qa_predictions(dataset_train, dataset_train_tokenized,
  File "/home/gokul.kumar/Desktop/multilingual-qa/datasets_local.py", line 223, in postprocess_qa_predictions
    end_char = offset_mapping[end_index][1]
IndexError: list index out of range